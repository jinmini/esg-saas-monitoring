"""
í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ë„¤ì´ë²„ API í‚¤ ì„¤ì • í›„ ì‹¤í–‰í•˜ì—¬ í¬ë¡¤ë§ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""
import asyncio
import sys
import os

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.crawler.service import CrawlerService
from src.crawler.scrapers.news_scraper import NaverNewsScraper
from src.core.config import settings


async def test_naver_api():
    """ë„¤ì´ë²„ API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print("ğŸ” ë„¤ì´ë²„ API ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # API í‚¤ í™•ì¸
    if settings.NAVER_CLIENT_ID == "your-naver-client-id":
        print("âŒ ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env.dev íŒŒì¼ì— NAVER_CLIENT_IDì™€ NAVER_CLIENT_SECRETì„ ì„¤ì •í•˜ì„¸ìš”.")
        return False
    
    scraper = NaverNewsScraper()
    
    try:
        # ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        result = await scraper.search_news("ESG", display=5, sort="date")
        
        if result and 'items' in result:
            print(f"âœ… API ì—°ê²° ì„±ê³µ! ê²€ìƒ‰ ê²°ê³¼: {len(result['items'])}ê°œ")
            print(f"   ì „ì²´ ê²°ê³¼ ìˆ˜: {result.get('total', 0)}ê°œ")
            
            # ì²« ë²ˆì§¸ ê¸°ì‚¬ ì •ë³´ ì¶œë ¥
            if result['items']:
                first_item = result['items'][0]
                print(f"   ì²« ë²ˆì§¸ ê¸°ì‚¬: {first_item['title'][:50]}...")
            
            return True
        else:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
    except Exception as e:
        print(f"âŒ API ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        return False


async def test_company_crawling():
    """íšŒì‚¬ë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ“° íšŒì‚¬ë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    crawler_service = CrawlerService()
    
    try:
        # í™œì„±í™”ëœ íšŒì‚¬ ëª©ë¡ ì¡°íšŒ
        companies = await crawler_service.get_active_companies()
        print(f"ğŸ“‹ í™œì„±í™”ëœ íšŒì‚¬ ìˆ˜: {len(companies)}")
        
        if not companies:
            print("âŒ í™œì„±í™”ëœ íšŒì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì²« ë²ˆì§¸ íšŒì‚¬ë¡œ í…ŒìŠ¤íŠ¸
        test_company = companies[0]
        print(f"ğŸ¢ í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: {test_company['company_name']}")
        
        result = await crawler_service.crawl_single_company(
            company_id=test_company['id'],
            max_articles=5  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 5ê°œë§Œ
        )
        
        if result.success:
            print(f"âœ… í¬ë¡¤ë§ ì„±ê³µ!")
            print(f"   - ê²€ìƒ‰ì–´: {result.query}")
            print(f"   - ë°œê²¬ëœ ê¸°ì‚¬ ìˆ˜: {result.total_found}")
            print(f"   - ì €ì¥ëœ ê¸°ì‚¬ ìˆ˜: {result.articles_saved}")
            print(f"   - ì†Œìš” ì‹œê°„: {result.crawl_duration:.2f}ì´ˆ")
        else:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {result.error_message}")
    
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")


async def test_crawl_statistics():
    """í¬ë¡¤ë§ í†µê³„ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ“Š í¬ë¡¤ë§ í†µê³„ ì¡°íšŒ í…ŒìŠ¤íŠ¸...")
    
    crawler_service = CrawlerService()
    
    try:
        stats = await crawler_service.get_crawl_statistics()
        
        print(f"ğŸ“ˆ í¬ë¡¤ë§ í†µê³„:")
        print(f"   - ì „ì²´ ê¸°ì‚¬ ìˆ˜: {stats['total_articles']}")
        print(f"   - íšŒì‚¬ ìˆ˜: {stats['companies_count']}")
        
        if stats['company_statistics']:
            print("   - íšŒì‚¬ë³„ ê¸°ì‚¬ ìˆ˜:")
            for company_name, count in stats['company_statistics'].items():
                print(f"     * {company_name}: {count}ê°œ")
        
    except Exception as e:
        print(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    # 1. API ì—°ê²° í…ŒìŠ¤íŠ¸
    api_ok = await test_naver_api()
    
    if not api_ok:
        print("\nâŒ API ì—°ê²°ì´ ì‹¤íŒ¨í•˜ì—¬ ì¶”ê°€ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return
    
    # 2. íšŒì‚¬ë³„ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
    await test_company_crawling()
    
    # 3. í†µê³„ ì¡°íšŒ í…ŒìŠ¤íŠ¸
    await test_crawl_statistics()
    
    print("\n" + "=" * 50)
    print("âœ… í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    asyncio.run(main())
