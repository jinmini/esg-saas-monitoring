from typing import List, Dict
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from loguru import logger

from .scrapers.news_scraper import NaverNewsScraper
from .schemas import CrawlResult, ArticleCreateRequest
from ..shared.models import Company, Article
from ..core.database import AsyncSessionLocal


class CrawlerService:
    """í¬ë¡¤ëŸ¬ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.scraper = NaverNewsScraper()
    
    async def get_active_companies(self) -> List[Dict]:
        """í™œì„±í™”ëœ íšŒì‚¬ ëª©ë¡ ì¡°íšŒ"""
        async with AsyncSessionLocal() as session:
            result = await session.execute(
                select(Company).where(Company.is_active == True)
            )
            companies = result.scalars().all()
            
            return [
                {
                    "id": company.id,
                    "company_name": company.company_name,
                    "company_name_en": company.company_name_en
                }
                for company in companies
            ]
    
    async def crawl_all_companies(self, max_articles_per_company: int = 50) -> List[CrawlResult]:
        """ëª¨ë“  í™œì„±í™”ëœ íšŒì‚¬ì˜ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        logger.info("Starting news crawling for all active companies")
        
        # í™œì„±í™”ëœ íšŒì‚¬ ëª©ë¡ ì¡°íšŒ
        companies = await self.get_active_companies()
        logger.info(f"Found {len(companies)} active companies")
        
        if not companies:
            logger.warning("No active companies found")
            return []
        
        # ê° íšŒì‚¬ë³„ ë‰´ìŠ¤ í¬ë¡¤ë§
        crawl_results = []
        for company in companies:
            try:
                result = await self.scraper.crawl_company_news(
                    company_id=company['id'],
                    company_name=company['company_name'],
                    max_articles=max_articles_per_company
                )
                
                # í¬ë¡¤ë§ëœ ê¸°ì‚¬ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ì¤‘ë³µ API í˜¸ì¶œ ì œê±°)
                if result.success and result.articles_data:
                    saved_count = await self.save_articles(result.articles_data)
                    result.articles_saved = saved_count
                    logger.info(f"Saved {saved_count} articles for {company['company_name']}")
                elif result.success:
                    logger.warning(f"No articles data returned for {company['company_name']}")
                else:
                    logger.error(f"Crawl failed for {company['company_name']}: {result.error_message}")
                
                crawl_results.append(result)
                logger.info(f"Crawled {company['company_name']}: {result.articles_saved} articles")
                
            except Exception as e:
                logger.error(f"Failed to crawl {company['company_name']}: {str(e)}")
                continue
        
        total_articles = sum(result.articles_saved for result in crawl_results if result.success)
        logger.info(f"Crawling completed. Total articles saved: {total_articles}")
        
        return crawl_results
    
    async def crawl_single_company(self, company_id: int, max_articles: int = 50) -> CrawlResult:
        """íŠ¹ì • íšŒì‚¬ì˜ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        async with AsyncSessionLocal() as session:
            # íšŒì‚¬ ì •ë³´ ì¡°íšŒ
            result = await session.execute(
                select(Company).where(Company.id == company_id, Company.is_active == True)
            )
            company = result.scalar_one_or_none()
            
            if not company:
                raise ValueError(f"Active company not found with ID: {company_id}")
            
            # ë‰´ìŠ¤ í¬ë¡¤ë§
            crawl_result = await self.scraper.crawl_company_news(
                company_id=company.id,
                company_name=company.company_name,
                max_articles=max_articles
            )
            
            # ì„±ê³µí•œ ê²½ìš° ê¸°ì‚¬ ì €ì¥ (ì¤‘ë³µ API í˜¸ì¶œ ì œê±°)
            if crawl_result.success and crawl_result.articles_data:
                saved_count = await self.save_articles(crawl_result.articles_data)
                crawl_result.articles_saved = saved_count
            
            return crawl_result
    
    async def save_articles(self, articles_data: List[Dict]) -> int:
        """ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (3ë‹¨ê³„ Quality Gate ì ìš©)"""
        if not articles_data:
            return 0
        
        async with AsyncSessionLocal() as session:
            saved_count = 0
            quality_filtered_count = 0
            
            for article_data in articles_data:
                try:
                    # ì¤‘ë³µ ê¸°ì‚¬ í™•ì¸ (URL ê¸°ì¤€)
                    existing = await session.execute(
                        select(Article).where(Article.article_url == article_data['article_url'])
                    )
                    
                    if existing.scalar_one_or_none():
                        logger.debug(f"Article already exists: {article_data['article_url']}")
                        continue
                    
                    # ğŸ›¡ï¸ 3ë‹¨ê³„ ë°©ì–´: Quality Gate - ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚°
                    relevance_score = await self._calculate_relevance_score(article_data)
                    
                    # Quality Gate: ìµœì†Œ ì ìˆ˜ ë¯¸ë‹¬ ì‹œ ì €ì¥ ê±°ë¶€
                    min_quality_score = 0.6  # 60% ì´ìƒì˜ ê´€ë ¨ì„± ìš”êµ¬
                    if relevance_score < min_quality_score:
                        quality_filtered_count += 1
                        logger.debug(f"Quality Gate blocked: '{article_data['title']}' (score: {relevance_score:.2f})")
                        continue
                    
                    # ìƒˆ ê¸°ì‚¬ ìƒì„±
                    # TODO: í–¥í›„ relevance_score í•„ë“œ ì¶”ê°€ ì‹œ ì €ì¥ ì˜ˆì •
                    article = Article(**article_data)
                    session.add(article)
                    saved_count += 1
                    
                except Exception as e:
                    logger.error(f"Failed to save article: {str(e)}")
                    continue
            
            try:
                await session.commit()
                logger.info(f"Saved {saved_count} new articles to database")
                if quality_filtered_count > 0:
                    logger.info(f"ğŸ›¡ï¸ Quality Gate blocked {quality_filtered_count} low-quality articles")
                return saved_count
                
            except Exception as e:
                await session.rollback()
                logger.error(f"Failed to commit articles: {str(e)}")
                return 0
    
    def _has_exact_word_match(self, text: str, keyword: str) -> bool:
        """ì •í™•í•œ ë‹¨ì–´ ê²½ê³„ ë§¤ì¹­ (PRD ì œì•ˆ ë°˜ì˜)"""
        import re
        if not text or not keyword:
            return False
        
        # ë‹¨ì–´ ê²½ê³„ë¥¼ ê³ ë ¤í•œ ì •í™•í•œ ë§¤ì¹­
        pattern = r'\b' + re.escape(keyword.strip()) + r'\b'
        return bool(re.search(pattern, text, re.IGNORECASE))
    
    def _calculate_context_score(self, title: str, summary: str) -> float:
        """ë¹„ì¦ˆë‹ˆìŠ¤/ESG ì»¨í…ìŠ¤íŠ¸ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)"""
        full_text = f"{title} {summary}".lower()
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜: 1)
        business_keywords = [
            "ê¸°ì—…", "íšŒì‚¬", "ì†”ë£¨ì…˜", "í”Œë«í¼", "ì„œë¹„ìŠ¤", "CEO", "ëŒ€í‘œ",
            "ìŠ¤íƒ€íŠ¸ì—…", "ê¸°ì—…ê°€", "ì°½ì—…", "íˆ¬ì", "ì‚¬ì—…", "ê²½ì˜"
        ]
        
        # ESG ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜: 2)
        esg_keywords = [
            "ESG", "íƒ„ì†Œ", "í™˜ê²½", "ì§€ì†ê°€ëŠ¥", "ì¹œí™˜ê²½", "ë…¹ìƒ‰", "ê¸°í›„",
            "ë°°ì¶œê¶Œ", "ë„·ì œë¡œ", "íƒ„ì†Œì¤‘ë¦½", "ì¬ìƒì—ë„ˆì§€", "LCA"
        ]
        
        business_score = sum(1 for kw in business_keywords if kw in full_text)
        esg_score = sum(2 for kw in esg_keywords if kw in full_text)  # ESG í‚¤ì›Œë“œ 2ë°° ê°€ì¤‘ì¹˜
        
        # ìµœëŒ€ ì ìˆ˜ ì •ê·œí™” (ë¹„ì¦ˆë‹ˆìŠ¤ 3ê°œ + ESG 2ê°œ = 7ì  ë§Œì )
        max_possible_score = 7.0
        total_score = min(business_score + esg_score, max_possible_score)
        
        context_score = total_score / max_possible_score
        logger.debug(f"Context score: {context_score:.2f} (business: {business_score}, esg: {esg_score})")
        
        return context_score

    async def _calculate_relevance_score(self, article_data: Dict) -> float:
        """ê°œì„ ëœ ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚° (ì •í™•í•œ ë§¤ì¹­ + ì»¨í…ìŠ¤íŠ¸ ê³ ë ¤)"""
        try:
            company_id = article_data.get('company_id')
            title = article_data.get('title', '')
            summary = article_data.get('summary', '')
            
            if not company_id:
                return 0.0
            
            # DBì—ì„œ íšŒì‚¬ ì •ë³´ ì¡°íšŒ
            async with AsyncSessionLocal() as session:
                result = await session.execute(
                    select(Company.company_name, Company.company_name_en, Company.positive_keywords, Company.negative_keywords)
                    .where(Company.id == company_id)
                )
                row = result.first()
                
                if not row:
                    return 0.0
                
                company_name = row.company_name
                company_name_en = row.company_name_en or ''
                positive_keywords = row.positive_keywords or []
                negative_keywords = row.negative_keywords or []
                
                full_text = f"{title} {summary}"
                score = 0.0
                
                # 1. íšŒì‚¬ëª… ì •í™• ë§¤ì¹­ (ê°€ì¤‘ì¹˜: 35%)
                if self._has_exact_word_match(title, company_name):
                    score += 0.35
                elif self._has_exact_word_match(summary, company_name):
                    score += 0.20
                
                # 2. ì˜ì–´ íšŒì‚¬ëª… ì •í™• ë§¤ì¹­ (ê°€ì¤‘ì¹˜: 25%)
                if company_name_en and self._has_exact_word_match(full_text, company_name_en):
                    score += 0.25
                
                # 3. Positive keywords ì •í™• ë§¤ì¹­ (ê°€ì¤‘ì¹˜: 20%)
                positive_matches = sum(1 for kw in positive_keywords 
                                     if self._has_exact_word_match(full_text, kw))
                if positive_keywords:
                    positive_ratio = min(positive_matches / len(positive_keywords), 1.0)
                    score += 0.2 * positive_ratio
                
                # 4. ì»¨í…ìŠ¤íŠ¸ ì ìˆ˜ (ê°€ì¤‘ì¹˜: 20%)
                context_score = self._calculate_context_score(title, summary)
                score += 0.2 * context_score
                
                # 5. Negative keywords íŒ¨ë„í‹° (-60%)
                for neg_kw in negative_keywords:
                    if self._has_exact_word_match(full_text, neg_kw):
                        score -= 0.6
                        logger.debug(f"Negative keyword penalty: '{neg_kw}' found in article")
                        break
                
                # ì ìˆ˜ ì •ê·œí™” (0.0 ~ 1.0)
                final_score = max(0.0, min(1.0, score))
                
                logger.debug(f"Enhanced relevance score for '{title[:50]}...': {final_score:.2f}")
                return final_score
                
        except Exception as e:
            logger.error(f"Failed to calculate enhanced relevance score: {e}")
            return 0.5  # ì—ëŸ¬ ì‹œ ì¤‘ê°„ê°’ ë°˜í™˜
    
    async def get_crawl_statistics(self) -> Dict:
        """í¬ë¡¤ë§ í†µê³„ ì •ë³´ ì¡°íšŒ"""
        async with AsyncSessionLocal() as session:
            # ì „ì²´ ê¸°ì‚¬ ìˆ˜
            total_articles = await session.execute(select(Article))
            total_count = len(total_articles.scalars().all())
            
            # íšŒì‚¬ë³„ ê¸°ì‚¬ ìˆ˜ (ì •í™•í•œ ì§‘ê³„)
            from sqlalchemy import func
            companies_with_articles = await session.execute(
                select(Company.company_name, func.count(Article.id).label('article_count'))
                .join(Article)
                .group_by(Company.id, Company.company_name)
            )
            
            company_stats = {}
            for company_name, article_count in companies_with_articles:
                company_stats[company_name] = article_count
            
            return {
                "total_articles": total_count,
                "companies_count": len(company_stats),
                "company_statistics": company_stats
            }
