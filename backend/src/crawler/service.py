from typing import List, Dict
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func
from loguru import logger
import re

from .scrapers.news_scraper import NaverNewsScraper
from .schemas import CrawlResult, ArticleCreateRequest
from ..shared.models import Company, Article
from ..core.database import AsyncSessionLocal
from .constants import PRECISION_SCORE_BOOST


class CrawlerService:
    """í¬ë¡¤ëŸ¬ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.scraper = NaverNewsScraper()
    
    async def get_active_companies(self) -> List[Dict]:
        """í™œì„±í™”ëœ íšŒì‚¬ ëª©ë¡ ì¡°íšŒ"""
        async with AsyncSessionLocal() as session:
            result = await session.execute(
                select(Company).where(Company.is_active == True)
            )
            companies = result.scalars().all()
            
            return [
                {
                    "id": company.id,
                    "company_name": company.company_name,
                    "company_name_en": company.company_name_en
                }
                for company in companies
            ]
    
    async def crawl_all_companies(self, max_articles_per_company: int = 50) -> List[CrawlResult]:
        """ëª¨ë“  í™œì„±í™”ëœ íšŒì‚¬ì˜ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        logger.info("Starting news crawling for all active companies")
        
        # í™œì„±í™”ëœ íšŒì‚¬ ëª©ë¡ ì¡°íšŒ
        companies = await self.get_active_companies()
        logger.info(f"Found {len(companies)} active companies")
        
        if not companies:
            logger.warning("No active companies found")
            return []
        
        # ê° íšŒì‚¬ë³„ ë‰´ìŠ¤ í¬ë¡¤ë§
        crawl_results = []
        for company in companies:
            try:
                result = await self.scraper.crawl_company_news(
                    company_id=company['id'],
                    company_name=company['company_name'],
                    max_articles=max_articles_per_company
                )
                
                # í¬ë¡¤ë§ëœ ê¸°ì‚¬ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                if result.success and result.articles_data:
                    saved_count = await self.save_articles(result.articles_data)
                    result.articles_saved = saved_count
                    logger.info(f"Saved {saved_count} articles for {company['company_name']}")
                elif result.success:
                    logger.warning(f"No articles data returned for {company['company_name']}")
                else:
                    logger.error(f"Crawl failed for {company['company_name']}: {result.error_message}")
                
                crawl_results.append(result)
                
            except Exception as e:
                logger.error(f"Failed to crawl {company['company_name']}: {str(e)}")
                continue
        
        total_articles = sum(result.articles_saved for result in crawl_results if result.success)
        logger.info(f"Crawling completed. Total articles saved: {total_articles}")
        
        return crawl_results
    
    async def crawl_single_company(self, company_id: int, max_articles: int = 50) -> CrawlResult:
        """íŠ¹ì • íšŒì‚¬ì˜ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        async with AsyncSessionLocal() as session:
            result = await session.execute(
                select(Company).where(Company.id == company_id, Company.is_active == True)
            )
            company = result.scalar_one_or_none()
            
            if not company:
                raise ValueError(f"Active company not found with ID: {company_id}")
            
            crawl_result = await self.scraper.crawl_company_news(
                company_id=company.id,
                company_name=company.company_name,
                max_articles=max_articles
            )
            
            if crawl_result.success and crawl_result.articles_data:
                saved_count = await self.save_articles(crawl_result.articles_data)
                crawl_result.articles_saved = saved_count
            
            return crawl_result
    
    async def save_articles(self, articles_data: List[Dict]) -> int:
        """ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (3ë‹¨ê³„ Quality Gate ì ìš©)"""
        if not articles_data:
            return 0
        
        # âœ… [Refactor] ì„¸ì…˜ì„ í•œ ë²ˆë§Œ ì—´ê³  ë£¨í”„ ë‚´ì—ì„œ ì¬ì‚¬ìš©
        async with AsyncSessionLocal() as session:
            saved_count = 0
            quality_filtered_count = 0
            
            for article_data in articles_data:
                try:
                    # ì¤‘ë³µ ê¸°ì‚¬ í™•ì¸ (URL ê¸°ì¤€)
                    existing = await session.execute(
                        select(Article).where(Article.article_url == article_data['article_url'])
                    )
                    
                    if existing.scalar_one_or_none():
                        logger.debug(f"Article already exists: {article_data['article_url']}")
                        continue
                    
                    # ğŸ›¡ï¸ 3ë‹¨ê³„ ë°©ì–´: Quality Gate - ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚°
                    # âœ… [Refactor] ì„¸ì…˜ ê°ì²´ ì „ë‹¬ (DB ì—°ê²° ì˜¤ë²„í—¤ë“œ ì œê±°)
                    relevance_score = await self._calculate_relevance_score(session, article_data)
                    
                    min_quality_score = 0.6
                    if relevance_score < min_quality_score:
                        quality_filtered_count += 1
                        logger.debug(f"Quality Gate blocked: '{article_data.get('title')}' (score: {relevance_score:.2f})")
                        continue
                    
                    # ìƒˆ ê¸°ì‚¬ ìƒì„±
                    # ë‚´ë¶€ ë©”íƒ€ë°ì´í„°('_'ë¡œ ì‹œì‘) ì œê±°
                    sanitized = {k: v for k, v in article_data.items() if not str(k).startswith('_')}
                    
                    # âœ… [Update] image_url í¬í•¨í•˜ì—¬ ê°ì²´ ìƒì„±
                    article = Article(
                        company_id=sanitized.get('company_id'),
                        title=sanitized.get('title'),
                        source_name=sanitized.get('source_name'),
                        article_url=sanitized.get('article_url'),
                        published_at=sanitized.get('published_at'),
                        content=sanitized.get('content'),
                        summary=sanitized.get('summary'),
                        image_url=sanitized.get('image_url'),  # ì´ë¯¸ì§€ URL ì €ì¥
                        # í•„ìš”í•œ ê²½ìš° ì¶”ê°€ í•„ë“œ ë§¤í•‘
                    )
                    
                    session.add(article)
                    saved_count += 1
                    
                except Exception as e:
                    logger.error(f"Failed to save article: {str(e)}")
                    continue
            
            try:
                await session.commit()
                if saved_count > 0:
                    logger.info(f"Saved {saved_count} new articles to database")
                if quality_filtered_count > 0:
                    logger.info(f"ğŸ›¡ï¸ Quality Gate blocked {quality_filtered_count} low-quality articles")
                return saved_count
                
            except Exception as e:
                await session.rollback()
                logger.error(f"Failed to commit articles: {str(e)}")
                return 0
    
    def _has_exact_word_match(self, text: str, keyword: str) -> bool:
        """ì •í™•í•œ ë‹¨ì–´ ê²½ê³„ ë§¤ì¹­"""
        if not text or not keyword:
            return False
        
        # íŠ¹ìˆ˜ë¬¸ì ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
        pattern = r'\b' + re.escape(keyword.strip()) + r'\b'
        return bool(re.search(pattern, text, re.IGNORECASE))
    
    def _calculate_context_score(self, title: str, summary: str) -> float:
        """ë¹„ì¦ˆë‹ˆìŠ¤/ESG ì»¨í…ìŠ¤íŠ¸ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)"""
        full_text = f"{title} {summary}".lower()
        
        business_keywords = [
            "ê¸°ì—…", "íšŒì‚¬", "ì†”ë£¨ì…˜", "í”Œë«í¼", "ì„œë¹„ìŠ¤", "CEO", "ëŒ€í‘œ",
            "ìŠ¤íƒ€íŠ¸ì—…", "ê¸°ì—…ê°€", "ì°½ì—…", "íˆ¬ì", "ì‚¬ì—…", "ê²½ì˜"
        ]
        
        esg_keywords = [
            "ESG", "íƒ„ì†Œ", "í™˜ê²½", "ì§€ì†ê°€ëŠ¥", "ì¹œí™˜ê²½", "ë…¹ìƒ‰", "ê¸°í›„",
            "ë°°ì¶œê¶Œ", "ë„·ì œë¡œ", "íƒ„ì†Œì¤‘ë¦½", "ì¬ìƒì—ë„ˆì§€", "LCA"
        ]
        
        business_score = sum(1 for kw in business_keywords if kw in full_text)
        esg_score = sum(2 for kw in esg_keywords if kw in full_text)
        
        max_possible_score = 7.0
        total_score = min(business_score + esg_score, max_possible_score)
        
        return total_score / max_possible_score

    async def _calculate_relevance_score(self, session: AsyncSession, article_data: Dict) -> float:
        """ê°œì„ ëœ ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚° (ì„¸ì…˜ ì¬ì‚¬ìš© ë²„ì „)"""
        try:
            company_id = article_data.get('company_id')
            title = article_data.get('title', '')
            summary = article_data.get('summary', '')
            
            if not company_id:
                return 0.0
            
            # âœ… [Refactor] ì „ë‹¬ë°›ì€ session ì‚¬ìš© (ìƒˆ ì—°ê²° ì•ˆ ë§Œë“¦)
            result = await session.execute(
                select(Company.company_name, Company.company_name_en, Company.positive_keywords, Company.negative_keywords)
                .where(Company.id == company_id)
            )
            row = result.first()
            
            if not row:
                return 0.0
            
            company_name = row.company_name
            company_name_en = row.company_name_en or ''
            positive_keywords = row.positive_keywords or []
            negative_keywords = row.negative_keywords or []
            
            full_text = f"{title} {summary}"
            score = 0.0
            
            # 1. íšŒì‚¬ëª… ì •í™• ë§¤ì¹­
            title_has_company = self._has_exact_word_match(title, company_name)
            summary_has_company = self._has_exact_word_match(summary, company_name)
            if title_has_company:
                score += 0.35
            elif summary_has_company:
                score += 0.20
            else:
                score -= 0.10
            
            # 2. ì˜ì–´ íšŒì‚¬ëª… ì •í™• ë§¤ì¹­
            if company_name_en and self._has_exact_word_match(full_text, company_name_en):
                score += 0.25
            
            # 3. Positive keywords ì •í™• ë§¤ì¹­
            if positive_keywords:
                positive_matches = sum(1 for kw in positive_keywords 
                                     if self._has_exact_word_match(full_text, kw))
                positive_ratio = min(positive_matches / len(positive_keywords), 1.0)
                score += 0.2 * positive_ratio
            
            # 4. ì»¨í…ìŠ¤íŠ¸ ì ìˆ˜
            context_score = self._calculate_context_score(title, summary)
            score += 0.2 * context_score
            
            # 5. Negative keywords íŒ¨ë„í‹°
            for neg_kw in negative_keywords:
                if self._has_exact_word_match(full_text, neg_kw):
                    score -= 0.6
                    break
            
            # 6. ì •ë°€ íŠ¸ë™ ê°€ì‚°ì 
            try:
                if article_data.get('_source_track') == 'precision':
                    score += PRECISION_SCORE_BOOST
            except Exception:
                pass

            final_score = max(0.0, min(1.0, score))
            return final_score
                
        except Exception as e:
            logger.error(f"Failed to calculate enhanced relevance score: {e}")
            return 0.5  # ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’
    
    async def get_crawl_statistics(self) -> Dict:
        """í¬ë¡¤ë§ í†µê³„ ì •ë³´ ì¡°íšŒ"""
        async with AsyncSessionLocal() as session:
            total_articles = await session.execute(select(func.count(Article.id)))
            total_count = total_articles.scalar()
            
            companies_with_articles = await session.execute(
                select(Company.company_name, func.count(Article.id).label('article_count'))
                .join(Article)
                .group_by(Company.id, Company.company_name)
            )
            
            company_stats = {name: count for name, count in companies_with_articles}
            
            return {
                "total_articles": total_count,
                "companies_count": len(company_stats),
                "company_statistics": company_stats
            }