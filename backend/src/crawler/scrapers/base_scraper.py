from abc import ABC, abstractmethod
from typing import List, Optional, Dict
import httpx
import asyncio
from datetime import datetime
import re
from loguru import logger

from ..schemas import CrawlResult
from ...core.config import settings
from ..constants import (
    TWO_TRACK_ENABLED,
    PRECISION_MIN_RESULTS,
    BROAD_MAX_PAGES,
    DISPLAY_PER_PAGE,
    DEFAULT_SEARCH_STRATEGY,
)


class BaseScraper(ABC):
    """í¬ë¡¤ëŸ¬ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.client_id = settings.NAVER_CLIENT_ID
        self.client_secret = settings.NAVER_CLIENT_SECRET
        self.delay = settings.CRAWLER_DELAY_SECONDS
        
    @abstractmethod
    async def search_news(self, query: str, display: int = 10, start: int = 1, sort: str = "sim") -> dict:
        """ë‰´ìŠ¤ ê²€ìƒ‰ ì¶”ìƒ ë©”ì„œë“œ"""
        pass
    
    @abstractmethod
    async def parse_articles(
        self,
        response_data: dict,
        company_id: int,
        company_name: str = None,
        source_track: Optional[str] = None,
        query_used: Optional[str] = None,
    ) -> List[dict]:
        """ë‰´ìŠ¤ ë°ì´í„° íŒŒì‹± ì¶”ìƒ ë©”ì„œë“œ"""
        pass
    
    async def crawl_company_news(self, company_id: int, company_name: str, max_articles: int = 100) -> CrawlResult:
        """íŠ¹ì • íšŒì‚¬ì˜ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§ (Two-Track ì „ëµ ì§€ì›)"""
        start_time = datetime.now()
        
        try:
            logger.info(f"Starting crawl for company: {company_name} (ID: {company_id})")

            # Two-Track ì¿¼ë¦¬ êµ¬ì„± ì‹œë„
            use_two_track = False
            queries: Dict[str, List[str]] = {"precision": [], "broad": []}
            try:
                if TWO_TRACK_ENABLED:
                    queries = await self._build_two_track_queries(company_id, company_name)
                    use_two_track = bool(queries.get("precision") or queries.get("broad"))
            except Exception as e:
                logger.warning(f"Two-Track query build failed, fallback to single query: {e}")
                use_two_track = False
            
            all_articles: List[dict] = []
            total_found = 0

            if use_two_track:
                logger.info(f"Using Two-Track crawling for {company_name}")
                # 1) ì •ë°€ íŠ¸ë™: ëŒ€í‘œëª…/íƒ‘í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ ì¶©ë¶„ëŸ‰ í™•ë³´ ì‹œ ê´‘ì—­ ìƒëµ
                precision_collected = 0
                for q in queries.get("precision", []):
                    # ì²« ìš”ì²­ìœ¼ë¡œ total íŒŒì•…
                    initial = await self.search_news(q, display=DISPLAY_PER_PAGE, start=1, sort="date")
                    q_total = max(0, int(initial.get("total", 0)))
                    total_found += q_total

                    max_pages = max(1, (q_total + DISPLAY_PER_PAGE - 1) // DISPLAY_PER_PAGE)
                    page = 1
                    while page <= max_pages and len(all_articles) < max_articles and precision_collected < PRECISION_MIN_RESULTS:
                        start = 1 + (page - 1) * DISPLAY_PER_PAGE
                        resp = await self.search_news(q, display=DISPLAY_PER_PAGE, start=start, sort="date")
                        articles = await self.parse_articles(resp, company_id, company_name, source_track="precision", query_used=q)
                        all_articles.extend(articles)
                        precision_collected += len(articles)
                        page += 1
                        if page <= max_pages:
                            await asyncio.sleep(self.delay)

                    if precision_collected >= PRECISION_MIN_RESULTS:
                        logger.info(f"Precision track collected {precision_collected} (>= {PRECISION_MIN_RESULTS}), skipping broad track for now")
                        break

                # 2) ê´‘ì—­ íŠ¸ë™: í•„ìš” ì‹œ ìµœëŒ€ Ní˜ì´ì§€ ë³´ê°•
                if precision_collected < PRECISION_MIN_RESULTS:
                    for q in queries.get("broad", []):
                        for page in range(1, BROAD_MAX_PAGES + 1):
                            if len(all_articles) >= max_articles:
                                break
                            start = 1 + (page - 1) * DISPLAY_PER_PAGE
                            resp = await self.search_news(q, display=DISPLAY_PER_PAGE, start=start, sort="date")
                            articles = await self.parse_articles(resp, company_id, company_name, source_track="broad", query_used=q)
                            all_articles.extend(articles)
                            if page < BROAD_MAX_PAGES:
                                await asyncio.sleep(self.delay)
            else:
                # ë‹¨ì¼ ì¿¼ë¦¬ ì „ëµ (ê¸°ì¡´ ë¡œì§)
                query = await self._build_enhanced_query(company_id, company_name)
                initial_response = await self.search_news(query, display=10, start=1, sort="date")
                total_found = initial_response.get('total', 0)
                logger.info(f"Found {total_found} articles for {company_name}")
                articles_to_collect = min(total_found, max_articles)
                for start in range(1, articles_to_collect + 1, 10):
                    display = min(10, articles_to_collect - start + 1)
                    response = await self.search_news(query, display=display, start=start, sort="date")
                    articles = await self.parse_articles(response, company_id, company_name, source_track="single", query_used=query)
                    all_articles.extend(articles)
                    if start + 10 <= articles_to_collect:
                        await asyncio.sleep(self.delay)
            
            # ì¤‘ë³µ ì œê±° (URL ì •ê·œí™” + ë³´ì¡° í‚¤ ê¸°ì¤€)
            unique_articles = self._dedupe_articles(all_articles)
            duration = (datetime.now() - start_time).total_seconds()
            
            return CrawlResult(
                company_id=company_id,
                company_name=company_name,
                query="two_track" if use_two_track else (query if 'query' in locals() else company_name),
                total_found=total_found,
                articles_saved=len(unique_articles),
                success=True,
                crawl_duration=duration,
                articles_data=unique_articles
            )
            
        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds()
            logger.error(f"Crawl failed for {company_name}: {str(e)}")
            
            return CrawlResult(
                company_id=company_id,
                company_name=company_name,
                query=company_name,
                total_found=0,
                articles_saved=0,
                success=False,
                error_message=str(e),
                crawl_duration=duration,
                articles_data=[]
            )
    
    async def _build_enhanced_query(self, company_id: int, company_name: str) -> str:
        """DBì—ì„œ ê°€ì ¸ì˜¨ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•ë„ ìµœì í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        try:
            from src.core.database import AsyncSessionLocal
            from src.shared.models import Company
            from sqlalchemy import select
            
            async with AsyncSessionLocal() as session:
                # DBì—ì„œ íšŒì‚¬ì˜ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¡°íšŒ
                result = await session.execute(
                    select(Company.positive_keywords, Company.search_strategy)
                    .where(Company.id == company_id)
                )
                row = result.first()
                
                if row and row.positive_keywords:
                    positive_keywords = row.positive_keywords
                    search_strategy = row.search_strategy or 'enhanced'
                    
                    if search_strategy == 'enhanced' and len(positive_keywords) > 0:
                        # ğŸ¯ í•µì‹¬ ì „ëµ: ë°˜ë“œì‹œ í¬í•¨í•  í‚¤ì›Œë“œë“¤ì„ ë”°ì˜´í‘œë¡œ ê°ì‹¸ê¸°
                        main_keywords = [f'"{kw}"' for kw in [company_name] + positive_keywords[:2]]
                        
                        # ê°€ì¥ ì •í™•í•œ í‚¤ì›Œë“œë¥¼ ë©”ì¸ìœ¼ë¡œ ì‚¬ìš© (ë³´í†µ íšŒì‚¬ëª…ì´ ê°€ì¥ ì •í™•)
                        primary_keyword = f'"{company_name}"'
                        
                        # ì¶”ê°€ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ OR ì¡°ê±´ìœ¼ë¡œ ê²°í•©í•˜ë˜, ë„¤ì´ë²„ API í•œê³„ë¡œ ê³µë°± ì‚¬ìš©
                        if len(positive_keywords) > 0:
                            # ì²« ë²ˆì§¸ positive_keywordë¥¼ ì¶”ê°€ (ì •í™•ë„ í–¥ìƒ)
                            secondary_keyword = f'"{positive_keywords[0]}"'
                            query = f"{primary_keyword} {secondary_keyword}"
                        else:
                            query = primary_keyword
                        
                        logger.info(f"Using precision-optimized query for {company_name}: {query}")
                        return query
                    else:
                        # ë‹¨ì¼ í‚¤ì›Œë“œ ì „ëµ - ê°€ì¥ ì •í™•í•œ ë§¤ì¹­
                        if positive_keywords:
                            query = f'"{positive_keywords[0]}"'
                        else:
                            query = f'"{company_name}"'
                        logger.info(f"Using exact match query for {company_name}: {query}")
                        return query
                else:
                    # DBì— í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ íšŒì‚¬ëª… ì •í™• ë§¤ì¹­
                    query = f'"{company_name}"'
                    logger.info(f"Using fallback exact match query for {company_name}: {query}")
                    return query
                    
        except Exception as e:
            logger.error(f"Failed to build query from DB for {company_name}: {e}")
            # ì—ëŸ¬ ì‹œ ê°€ì¥ ì•ˆì „í•œ ì •í™• ë§¤ì¹­ ì‚¬ìš©
            query = f'"{company_name}"'
            logger.info(f"Using fallback exact match query for {company_name}: {query}")
            return query

    async def _build_two_track_queries(self, company_id: int, company_name: str) -> Dict[str, List[str]]:
        """DB ë©”íƒ€(ceo_name, positive_keywords, search_strategy)ë¡œ Two-Track ì¿¼ë¦¬ êµ¬ì„±"""
        from src.core.database import AsyncSessionLocal
        from src.shared.models import Company
        from sqlalchemy import select

        precision: List[str] = []
        broad: List[str] = []

        async with AsyncSessionLocal() as session:
            result = await session.execute(
                select(
                    Company.ceo_name,
                    Company.positive_keywords,
                    Company.search_strategy,
                ).where(Company.id == company_id)
            )
            row = result.first()

        ceo_name = None
        positive_keywords: List[str] = []
        search_strategy = DEFAULT_SEARCH_STRATEGY
        if row:
            ceo_name = row.ceo_name
            positive_keywords = row.positive_keywords or []
            search_strategy = (row.search_strategy or DEFAULT_SEARCH_STRATEGY).lower()

        # ì „ëµì— ë”°ë¼ êµ¬ì„±
        if search_strategy in ("two_track", "precision_first", "enhanced"):
            # ì •ë°€ ì¿¼ë¦¬ë“¤
            if company_name and ceo_name:
                precision.append(f'"{company_name}" "{ceo_name}"')
            if company_name and positive_keywords:
                precision.append(f'"{company_name}" "{positive_keywords[0]}"')
            # ê´‘ì—­ ì¿¼ë¦¬
            if company_name:
                broad.append(f'"{company_name}"')
        elif search_strategy == "precision_only":
            if company_name and ceo_name:
                precision.append(f'"{company_name}" "{ceo_name}"')
            if company_name and positive_keywords:
                precision.append(f'"{company_name}" "{positive_keywords[0]}"')
        elif search_strategy == "broad_only":
            if company_name:
                broad.append(f'"{company_name}"')
        else:
            # ì•Œ ìˆ˜ ì—†ëŠ” ì „ëµì€ ì•ˆì „í•˜ê²Œ two_trackë¡œ ì²˜ë¦¬
            if company_name and ceo_name:
                precision.append(f'"{company_name}" "{ceo_name}"')
            if company_name and positive_keywords:
                precision.append(f'"{company_name}" "{positive_keywords[0]}"')
            if company_name:
                broad.append(f'"{company_name}"')

        # ì¤‘ë³µ ì œê±° ë° ê³µë°± ì œê±°
        def _dedupe(seq: List[str]) -> List[str]:
            seen = set()
            out = []
            for s in seq:
                s2 = (s or "").strip()
                if s2 and s2 not in seen:
                    seen.add(s2)
                    out.append(s2)
            return out

        queries = {
            "precision": _dedupe(precision),
            "broad": _dedupe(broad),
        }
        return queries
    
    def _has_exact_word_match(self, text: str, keyword: str) -> bool:
        """ì •í™•í•œ ë‹¨ì–´ ê²½ê³„ ë§¤ì¹­"""
        import re
        if not text or not keyword:
            return False
        
        # ë‹¨ì–´ ê²½ê³„ë¥¼ ê³ ë ¤í•œ ì •í™•í•œ ë§¤ì¹­
        pattern = r'\b' + re.escape(keyword.strip()) + r'\b'
        return bool(re.search(pattern, text, re.IGNORECASE))

    async def _is_relevant_article(self, title: str, content: str, company_id: int, company_name: str) -> bool:
        """ê°œì„ ëœ ê¸°ì‚¬ ê´€ë ¨ì„± ê²€ì¦ - ì •í™•í•œ ë„¤ê±°í‹°ë¸Œ í•„í„°ë§"""
        try:
            from src.core.database import AsyncSessionLocal
            from src.shared.models import Company
            from sqlalchemy import select
            
            async with AsyncSessionLocal() as session:
                # DBì—ì„œ íšŒì‚¬ì˜ ë„¤ê±°í‹°ë¸Œ í‚¤ì›Œë“œ ì¡°íšŒ
                result = await session.execute(
                    select(Company.negative_keywords)
                    .where(Company.id == company_id)
                )
                row = result.first()
                
                if row and row.negative_keywords:
                    negative_keywords = row.negative_keywords
                    
                    # ì „ì²´ í…ìŠ¤íŠ¸ í•©ì„±
                    full_text = f"{title} {content}"
                    
                    # ì •í™•í•œ ë„¤ê±°í‹°ë¸Œ í‚¤ì›Œë“œ ë§¤ì¹­
                    for neg_keyword in negative_keywords:
                        if self._has_exact_word_match(full_text, neg_keyword):
                            logger.debug(f"Article filtered out for {company_name} due to exact negative keyword match: {neg_keyword}")
                            return False
                
                return True
                
        except Exception as e:
            logger.error(f"Failed to check relevance from DB for {company_name}: {e}")
            # ì—ëŸ¬ ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ ê´€ë ¨ì„± ìˆë‹¤ê³  íŒë‹¨
            return True
    
    def _normalize_url(self, url: Optional[str]) -> Optional[str]:
        """URL ì •ê·œí™”: ìŠ¤í‚´/í˜¸ìŠ¤íŠ¸ ì†Œë¬¸ì, ì¶”ì  íŒŒë¼ë¯¸í„° ì œê±°, ì¿¼ë¦¬ ì •ë ¬"""
        if not url:
            return None
        try:
            from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode
            parsed = urlparse(url)
            scheme = (parsed.scheme or 'http').lower()
            netloc = (parsed.netloc or '').lower()
            path = parsed.path or ''
            # ì¿¼ë¦¬ ì •ë¦¬: UTM ë“± ì¶”ì  íŒŒë¼ë¯¸í„° ì œê±°
            query_pairs = [(k, v) for (k, v) in parse_qsl(parsed.query, keep_blank_values=False)
                           if not k.lower().startswith('utm_') and k.lower() not in {'gclid', 'fbclid'}]
            query = urlencode(sorted(query_pairs))
            fragment = ''
            return urlunparse((scheme, netloc, path, '', query, fragment))
        except Exception:
            return url

    def _dedupe_articles(self, articles: List[dict]) -> List[dict]:
        """URL ì •ê·œí™” ê¸°ë°˜ ì¤‘ë³µ ì œê±°, URL ì—†ìœ¼ë©´ (title, source) í‚¤ë¡œ ë³´ì¡° ì œê±°"""
        seen: set = set()
        unique_articles: List[dict] = []
        for article in articles:
            url = self._normalize_url(article.get('article_url'))
            if url:
                key = ("url", url)
            else:
                title = (article.get('title') or '').strip().lower()
                source = (article.get('source_name') or '').strip().lower()
                key = ("ts", title, source)
            if key in seen:
                continue
            seen.add(key)
            unique_articles.append(article)
        return unique_articles
    
    def _clean_html_tags(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        clean_text = re.sub(r'<[^>]+>', '', text)
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        clean_text = clean_text.replace('&lt;', '<')
        clean_text = clean_text.replace('&gt;', '>')
        clean_text = clean_text.replace('&amp;', '&')
        clean_text = clean_text.replace('&quot;', '"')
        clean_text = clean_text.replace('&#39;', "'")
        
        return clean_text.strip()
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """ë„¤ì´ë²„ API ë‚ ì§œ í˜•ì‹ì„ datetimeìœ¼ë¡œ ë³€í™˜"""
        try:
            # ë„¤ì´ë²„ API ë‚ ì§œ í˜•ì‹: "Mon, 15 Sep 2025 10:30:00 +0900"
            from dateutil.parser import parse
            return parse(date_str)
        except Exception as e:
            logger.warning(f"Failed to parse date: {date_str}, error: {e}")
            return None
