from abc import ABC, abstractmethod
from typing import List, Optional
import httpx
import asyncio
from datetime import datetime
import re
from loguru import logger

from ..schemas import CrawlResult
from ...core.config import settings


class BaseScraper(ABC):
    """í¬ë¡¤ëŸ¬ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.client_id = settings.NAVER_CLIENT_ID
        self.client_secret = settings.NAVER_CLIENT_SECRET
        self.delay = settings.CRAWLER_DELAY_SECONDS
        
    @abstractmethod
    async def search_news(self, query: str, display: int = 10, start: int = 1, sort: str = "sim") -> dict:
        """ë‰´ìŠ¤ ê²€ìƒ‰ ì¶”ìƒ ë©”ì„œë“œ"""
        pass
    
    @abstractmethod
    async def parse_articles(self, response_data: dict, company_id: int, company_name: str = None) -> List[dict]:
        """ë‰´ìŠ¤ ë°ì´í„° íŒŒì‹± ì¶”ìƒ ë©”ì„œë“œ"""
        pass
    
    async def crawl_company_news(self, company_id: int, company_name: str, max_articles: int = 100) -> CrawlResult:
        """íŠ¹ì • íšŒì‚¬ì˜ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§"""
        start_time = datetime.now()
        
        try:
            logger.info(f"Starting crawl for company: {company_name} (ID: {company_id})")
            
            # ê°œì„ ëœ ê²€ìƒ‰ì–´ ìƒì„± (DB ê¸°ë°˜)
            query = await self._build_enhanced_query(company_id, company_name)
            
            # ì²« ë²ˆì§¸ ê²€ìƒ‰ìœ¼ë¡œ ì „ì²´ ê²°ê³¼ ìˆ˜ í™•ì¸
            initial_response = await self.search_news(query, display=10, start=1, sort="date")
            total_found = initial_response.get('total', 0)
            
            logger.info(f"Found {total_found} articles for {company_name}")
            
            # ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜ ê²°ì •
            articles_to_collect = min(total_found, max_articles)
            all_articles = []
            
            # í˜ì´ì§€ë³„ë¡œ ë°ì´í„° ìˆ˜ì§‘
            for start in range(1, articles_to_collect + 1, 10):
                display = min(10, articles_to_collect - start + 1)
                
                response = await self.search_news(query, display=display, start=start, sort="date")
                articles = await self.parse_articles(response, company_id, company_name)
                all_articles.extend(articles)
                
                # API ìš”ì²­ ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•œ ì§€ì—°
                if start + 10 <= articles_to_collect:
                    await asyncio.sleep(self.delay)
            
            # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
            unique_articles = self._remove_duplicates(all_articles)
            
            duration = (datetime.now() - start_time).total_seconds()
            
            return CrawlResult(
                company_id=company_id,
                company_name=company_name,
                query=query,
                total_found=total_found,
                articles_saved=len(unique_articles),
                success=True,
                crawl_duration=duration,
                articles_data=unique_articles  # ì‹¤ì œ ê¸°ì‚¬ ë°ì´í„° í¬í•¨
            )
            
        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds()
            logger.error(f"Crawl failed for {company_name}: {str(e)}")
            
            return CrawlResult(
                company_id=company_id,
                company_name=company_name,
                query=company_name,
                total_found=0,
                articles_saved=0,
                success=False,
                error_message=str(e),
                crawl_duration=duration,
                articles_data=[]  # ì—ëŸ¬ ì‹œ ë¹ˆ ë°°ì—´
            )
    
    async def _build_enhanced_query(self, company_id: int, company_name: str) -> str:
        """DBì—ì„œ ê°€ì ¸ì˜¨ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•ë„ ìµœì í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        try:
            from src.core.database import AsyncSessionLocal
            from src.shared.models import Company
            from sqlalchemy import select
            
            async with AsyncSessionLocal() as session:
                # DBì—ì„œ íšŒì‚¬ì˜ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¡°íšŒ
                result = await session.execute(
                    select(Company.positive_keywords, Company.search_strategy)
                    .where(Company.id == company_id)
                )
                row = result.first()
                
                if row and row.positive_keywords:
                    positive_keywords = row.positive_keywords
                    search_strategy = row.search_strategy or 'enhanced'
                    
                    if search_strategy == 'enhanced' and len(positive_keywords) > 0:
                        # ğŸ¯ í•µì‹¬ ì „ëµ: ë°˜ë“œì‹œ í¬í•¨í•  í‚¤ì›Œë“œë“¤ì„ ë”°ì˜´í‘œë¡œ ê°ì‹¸ê¸°
                        main_keywords = [f'"{kw}"' for kw in [company_name] + positive_keywords[:2]]
                        
                        # ê°€ì¥ ì •í™•í•œ í‚¤ì›Œë“œë¥¼ ë©”ì¸ìœ¼ë¡œ ì‚¬ìš© (ë³´í†µ íšŒì‚¬ëª…ì´ ê°€ì¥ ì •í™•)
                        primary_keyword = f'"{company_name}"'
                        
                        # ì¶”ê°€ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ OR ì¡°ê±´ìœ¼ë¡œ ê²°í•©í•˜ë˜, ë„¤ì´ë²„ API í•œê³„ë¡œ ê³µë°± ì‚¬ìš©
                        if len(positive_keywords) > 0:
                            # ì²« ë²ˆì§¸ positive_keywordë¥¼ ì¶”ê°€ (ì •í™•ë„ í–¥ìƒ)
                            secondary_keyword = f'"{positive_keywords[0]}"'
                            query = f"{primary_keyword} {secondary_keyword}"
                        else:
                            query = primary_keyword
                        
                        logger.info(f"Using precision-optimized query for {company_name}: {query}")
                        return query
                    else:
                        # ë‹¨ì¼ í‚¤ì›Œë“œ ì „ëµ - ê°€ì¥ ì •í™•í•œ ë§¤ì¹­
                        if positive_keywords:
                            query = f'"{positive_keywords[0]}"'
                        else:
                            query = f'"{company_name}"'
                        logger.info(f"Using exact match query for {company_name}: {query}")
                        return query
                else:
                    # DBì— í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ íšŒì‚¬ëª… ì •í™• ë§¤ì¹­
                    query = f'"{company_name}"'
                    logger.info(f"Using fallback exact match query for {company_name}: {query}")
                    return query
                    
        except Exception as e:
            logger.error(f"Failed to build query from DB for {company_name}: {e}")
            # ì—ëŸ¬ ì‹œ ê°€ì¥ ì•ˆì „í•œ ì •í™• ë§¤ì¹­ ì‚¬ìš©
            query = f'"{company_name}"'
            logger.info(f"Using fallback exact match query for {company_name}: {query}")
            return query
    
    async def _is_relevant_article(self, title: str, content: str, company_id: int, company_name: str) -> bool:
        """DB ê¸°ë°˜ ê¸°ì‚¬ ê´€ë ¨ì„± ê²€ì¦ - ë„¤ê±°í‹°ë¸Œ í•„í„°ë§"""
        try:
            from src.core.database import AsyncSessionLocal
            from src.shared.models import Company
            from sqlalchemy import select
            
            async with AsyncSessionLocal() as session:
                # DBì—ì„œ íšŒì‚¬ì˜ ë„¤ê±°í‹°ë¸Œ í‚¤ì›Œë“œ ì¡°íšŒ
                result = await session.execute(
                    select(Company.negative_keywords)
                    .where(Company.id == company_id)
                )
                row = result.first()
                
                if row and row.negative_keywords:
                    negative_keywords = row.negative_keywords
                    
                    # ì „ì²´ í…ìŠ¤íŠ¸ í•©ì„± (ì†Œë¬¸ì ë³€í™˜)
                    full_text = f"{title} {content}".lower()
                    
                    # ë„¤ê±°í‹°ë¸Œ í‚¤ì›Œë“œ í™•ì¸
                    for neg_keyword in negative_keywords:
                        if neg_keyword.lower() in full_text:
                            logger.debug(f"Article filtered out for {company_name} due to negative keyword: {neg_keyword}")
                            return False
                
                return True
                
        except Exception as e:
            logger.error(f"Failed to check relevance from DB for {company_name}: {e}")
            # ì—ëŸ¬ ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ ê´€ë ¨ì„± ìˆë‹¤ê³  íŒë‹¨
            return True
    
    def _remove_duplicates(self, articles: List[dict]) -> List[dict]:
        """URL ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ê¸°ì‚¬ ì œê±°"""
        seen_urls = set()
        unique_articles = []
        
        for article in articles:
            url = article.get('article_url')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_articles.append(article)
        
        return unique_articles
    
    def _clean_html_tags(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        clean_text = re.sub(r'<[^>]+>', '', text)
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        clean_text = clean_text.replace('&lt;', '<')
        clean_text = clean_text.replace('&gt;', '>')
        clean_text = clean_text.replace('&amp;', '&')
        clean_text = clean_text.replace('&quot;', '"')
        clean_text = clean_text.replace('&#39;', "'")
        
        return clean_text.strip()
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """ë„¤ì´ë²„ API ë‚ ì§œ í˜•ì‹ì„ datetimeìœ¼ë¡œ ë³€í™˜"""
        try:
            # ë„¤ì´ë²„ API ë‚ ì§œ í˜•ì‹: "Mon, 15 Sep 2025 10:30:00 +0900"
            from dateutil.parser import parse
            return parse(date_str)
        except Exception as e:
            logger.warning(f"Failed to parse date: {date_str}, error: {e}")
            return None
