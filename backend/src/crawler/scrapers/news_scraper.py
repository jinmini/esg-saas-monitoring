import httpx
from typing import List, Dict
from datetime import datetime
from loguru import logger

from .base_scraper import BaseScraper
from ..schemas import NaverNewsResponse, NaverNewsItem
from ...core.config import settings


class NaverNewsScraper(BaseScraper):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        super().__init__()
        self.base_url = "https://openapi.naver.com/v1/search/news.json"
    
    async def search_news(self, query: str, display: int = 10, start: int = 1, sort: str = "sim") -> dict:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API í˜¸ì¶œ"""
        headers = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret,
            "User-Agent": "ESG-SaaS-Monitor/1.0"
        }
        
        params = {
            "query": query,
            "display": display,
            "start": start,
            "sort": sort
        }
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                response = await client.get(
                    self.base_url,
                    headers=headers,
                    params=params
                )
                
                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 400:
                    raise Exception(f"Bad Request: Invalid parameters")
                elif response.status_code == 401:
                    raise Exception(f"Unauthorized: Invalid API credentials")
                elif response.status_code == 403:
                    raise Exception(f"Forbidden: API access denied")
                elif response.status_code == 429:
                    raise Exception(f"Too Many Requests: API rate limit exceeded")
                elif response.status_code >= 500:
                    raise Exception(f"Server Error: {response.status_code}")
                else:
                    raise Exception(f"Unexpected status code: {response.status_code}")
                    
            except httpx.TimeoutException:
                raise Exception("Request timeout")
            except httpx.RequestError as e:
                raise Exception(f"Request error: {str(e)}")
    
    async def parse_articles(self, response_data: dict, company_id: int, company_name: str = None) -> List[dict]:
        """ë„¤ì´ë²„ API ì‘ë‹µì„ Article ëª¨ë¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (2ë‹¨ê³„ í•„í„°ë§ ì ìš©)"""
        try:
            # Pydantic ëª¨ë¸ë¡œ ê²€ì¦
            naver_response = NaverNewsResponse(**response_data)
            
            # DBì—ì„œ íšŒì‚¬ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
            company_meta = await self._get_company_metadata(company_id) if company_name else {}
            
            articles = []
            title_filtered_count = 0
            relevance_filtered_count = 0
            
            for item in naver_response.items:
                title = self._clean_html_tags(item.title)
                summary = self._clean_html_tags(item.description)
                
                # ğŸ›¡ï¸ 2ë‹¨ê³„ ë°©ì–´: ì œëª© ê¸°ë°˜ ì¦‰ì‹œ í•„í„°ë§ (ê°€ì¥ ê°•ë ¥í•œ í•„í„°)
                if company_name:
                    keywords_to_check = [company_name.lower()]
                    if company_meta.get('positive_keywords'):
                        keywords_to_check.extend([kw.lower() for kw in company_meta['positive_keywords']])
                    
                    # ì œëª©ì— íšŒì‚¬ëª…ì´ë‚˜ í•µì‹¬ í‚¤ì›Œë“œê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì¦‰ì‹œ ì œì™¸
                    if not any(keyword in title.lower() for keyword in keywords_to_check):
                        title_filtered_count += 1
                        logger.debug(f"Title-filtered: '{title}' - no matching keywords")
                        continue
                
                # ë„¤ê±°í‹°ë¸Œ í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ì„± ê²€ì¦ (ê¸°ì¡´ ë¡œì§)
                if company_name and not await self._is_relevant_article(title, summary, company_id, company_name):
                    relevance_filtered_count += 1
                    continue
                
                article_data = {
                    "company_id": company_id,
                    "title": title,
                    "source_name": self._extract_source_name(item.link),
                    "article_url": item.originallink or item.link,
                    "published_at": self._parse_date(item.pubDate),
                    "summary": summary,
                    "language": "ko",
                    "is_verified": False
                }
                articles.append(article_data)
            
            logger.info(f"Parsed {len(articles)} articles for {company_name} (company_id: {company_id})")
            if title_filtered_count > 0:
                logger.info(f"ğŸ›¡ï¸ Title-filtered out {title_filtered_count} articles for {company_name}")
            if relevance_filtered_count > 0:
                logger.info(f"ğŸ›¡ï¸ Relevance-filtered out {relevance_filtered_count} articles for {company_name}")
            
            return articles
            
        except Exception as e:
            logger.error(f"Failed to parse articles: {str(e)}")
            return []
    
    async def _get_company_metadata(self, company_id: int) -> dict:
        """DBì—ì„œ íšŒì‚¬ ë©”íƒ€ë°ì´í„° ì¡°íšŒ (positive_keywords, negative_keywords)"""
        try:
            from src.core.database import AsyncSessionLocal
            from src.shared.models import Company
            from sqlalchemy import select
            
            async with AsyncSessionLocal() as session:
                result = await session.execute(
                    select(Company.positive_keywords, Company.negative_keywords)
                    .where(Company.id == company_id)
                )
                row = result.first()
                
                if row:
                    return {
                        'positive_keywords': row.positive_keywords or [],
                        'negative_keywords': row.negative_keywords or []
                    }
                return {}
                
        except Exception as e:
            logger.error(f"Failed to get company metadata for ID {company_id}: {e}")
            return {}
    
    def _extract_source_name(self, link: str) -> str:
        """ë‰´ìŠ¤ ë§í¬ì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(link)
            domain = parsed.netloc
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë„ë©”ì¸ ë§¤í•‘
            source_mapping = {
                "news.naver.com": "ë„¤ì´ë²„ë‰´ìŠ¤",
                "www.chosun.com": "ì¡°ì„ ì¼ë³´",
                "www.donga.com": "ë™ì•„ì¼ë³´",
                "www.joongang.co.kr": "ì¤‘ì•™ì¼ë³´",
                "www.hankyung.com": "í•œêµ­ê²½ì œ",
                "www.mk.co.kr": "ë§¤ì¼ê²½ì œ",
                "www.etnews.com": "ì „ìì‹ ë¬¸",
                "biz.chosun.com": "ì¡°ì„ ë¹„ì¦ˆ",
                "www.sedaily.com": "ì„œìš¸ê²½ì œ",
                "www.fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤"
            }
            
            return source_mapping.get(domain, domain)
            
        except Exception:
            return "Unknown"
    
    async def crawl_multiple_companies(self, companies: List[Dict[str, any]], max_articles_per_company: int = 50) -> List[dict]:
        """ì—¬ëŸ¬ íšŒì‚¬ì˜ ë‰´ìŠ¤ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ë§"""
        results = []
        
        for company in companies:
            company_id = company['id']
            company_name = company['company_name']
            
            try:
                result = await self.crawl_company_news(
                    company_id=company_id,
                    company_name=company_name,
                    max_articles=max_articles_per_company
                )
                results.append(result)
                
                logger.info(f"Completed crawling for {company_name}: {result.articles_saved} articles")
                
            except Exception as e:
                logger.error(f"Failed to crawl {company_name}: {str(e)}")
                continue
        
        return results
