"""
ë²¡í„°ìŠ¤í† ì–´ ìë™ ê°±ì‹  ì‹œìŠ¤í…œ
JSONL íŒŒì¼ ë³€ê²½ ê°ì§€ ë° ì¬ì„ë² ë”©
"""
import logging
import hashlib
import time
from pathlib import Path
from typing import Dict, Any, Optional, Set
from datetime import datetime
import asyncio
import json

from .embed_pipeline import ESGEmbeddingPipeline, EmbeddingStats

logger = logging.getLogger(__name__)


class FileHashTracker:
    """íŒŒì¼ í•´ì‹œ ì¶”ì ê¸° (ë³€ê²½ ê°ì§€ìš©)"""
    
    def __init__(self, cache_file: Path = Path("./data/.file_hashes.json")):
        """
        Args:
            cache_file: í•´ì‹œ ìºì‹œ íŒŒì¼ ê²½ë¡œ
        """
        self.cache_file = cache_file
        self.hashes: Dict[str, str] = {}
        self.load_cache()
    
    def load_cache(self) -> None:
        """ìºì‹œ íŒŒì¼ ë¡œë“œ"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, "r", encoding="utf-8") as f:
                    self.hashes = json.load(f)
                logger.info(f"Loaded hash cache: {len(self.hashes)} files")
            except Exception as e:
                logger.error(f"Failed to load hash cache: {e}")
                self.hashes = {}
        else:
            logger.info("No hash cache found, starting fresh")
    
    def save_cache(self) -> None:
        """ìºì‹œ íŒŒì¼ ì €ì¥"""
        try:
            self.cache_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.cache_file, "w", encoding="utf-8") as f:
                json.dump(self.hashes, f, indent=2)
            logger.debug(f"Saved hash cache: {len(self.hashes)} files")
        except Exception as e:
            logger.error(f"Failed to save hash cache: {e}")
    
    def compute_hash(self, file_path: Path) -> str:
        """
        íŒŒì¼ í•´ì‹œ ê³„ì‚°
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            
        Returns:
            SHA256 í•´ì‹œ
        """
        sha256 = hashlib.sha256()
        
        try:
            with open(file_path, "rb") as f:
                while chunk := f.read(8192):
                    sha256.update(chunk)
            return sha256.hexdigest()
        except Exception as e:
            logger.error(f"Failed to compute hash for {file_path}: {e}")
            return ""
    
    def has_changed(self, file_path: Path) -> bool:
        """
        íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            
        Returns:
            True if changed
        """
        file_key = str(file_path)
        current_hash = self.compute_hash(file_path)
        
        if not current_hash:
            return False
        
        previous_hash = self.hashes.get(file_key)
        
        if previous_hash is None:
            # ìƒˆ íŒŒì¼
            logger.info(f"New file detected: {file_path.name}")
            self.hashes[file_key] = current_hash
            return True
        
        if previous_hash != current_hash:
            # ë³€ê²½ëœ íŒŒì¼
            logger.info(f"File changed: {file_path.name}")
            self.hashes[file_key] = current_hash
            return True
        
        return False
    
    def get_changed_files(self, file_paths: list[Path]) -> Set[Path]:
        """
        ë³€ê²½ëœ íŒŒì¼ ëª©ë¡ ë°˜í™˜
        
        Args:
            file_paths: ì²´í¬í•  íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë³€ê²½ëœ íŒŒì¼ ê²½ë¡œ ì§‘í•©
        """
        changed = set()
        
        for file_path in file_paths:
            if not file_path.exists():
                logger.warning(f"File not found: {file_path}")
                continue
            
            if self.has_changed(file_path):
                changed.add(file_path)
        
        return changed


class VectorRefreshTask:
    """
    ë²¡í„°ìŠ¤í† ì–´ ê°±ì‹  íƒœìŠ¤í¬
    ì£¼ê¸°ì ìœ¼ë¡œ JSONL íŒŒì¼ì„ ì²´í¬í•˜ê³  ë³€ê²½ ì‹œ ì¬ì„ë² ë”©
    """
    
    def __init__(
        self,
        data_dir: Path,
        chroma_persist_dir: Path = Path("./data/chroma"),
        collection_name: str = "esg_standards",
        check_interval: int = 3600,  # 1ì‹œê°„ë§ˆë‹¤ ì²´í¬
        auto_start: bool = False
    ):
        """
        Args:
            data_dir: JSONL íŒŒì¼ ë””ë ‰í† ë¦¬
            chroma_persist_dir: ChromaDB ì €ì¥ ê²½ë¡œ
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            check_interval: ì²´í¬ ê°„ê²© (ì´ˆ)
            auto_start: Trueë©´ ìë™ ì‹œì‘
        """
        self.data_dir = Path(data_dir)
        self.chroma_persist_dir = Path(chroma_persist_dir)
        self.collection_name = collection_name
        self.check_interval = check_interval
        
        # íŒŒì¼ ì¶”ì ê¸°
        cache_file = self.chroma_persist_dir / ".file_hashes.json"
        self.tracker = FileHashTracker(cache_file=cache_file)
        
        # ì„ë² ë”© íŒŒì´í”„ë¼ì¸
        self.pipeline = ESGEmbeddingPipeline(
            data_dir=data_dir,
            chroma_persist_dir=chroma_persist_dir,
            collection_name=collection_name
        )
        
        # ìƒíƒœ
        self.is_running = False
        self.last_check_time: Optional[datetime] = None
        self.refresh_count = 0
        
        logger.info(f"VectorRefreshTask initialized: check every {check_interval}s")
        
        if auto_start:
            asyncio.create_task(self.start())
    
    async def start(self) -> None:
        """ê°±ì‹  íƒœìŠ¤í¬ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ ë£¨í”„)"""
        if self.is_running:
            logger.warning("Refresh task already running")
            return
        
        self.is_running = True
        logger.info("ğŸ”„ Vector refresh task started")
        
        try:
            while self.is_running:
                await self.check_and_refresh()
                await asyncio.sleep(self.check_interval)
        except asyncio.CancelledError:
            logger.info("Refresh task cancelled")
        except Exception as e:
            logger.error(f"Refresh task error: {e}", exc_info=True)
        finally:
            self.is_running = False
    
    def stop(self) -> None:
        """ê°±ì‹  íƒœìŠ¤í¬ ì¤‘ì§€"""
        logger.info("Stopping vector refresh task...")
        self.is_running = False
    
    async def check_and_refresh(self) -> Dict[str, Any]:
        """
        íŒŒì¼ ë³€ê²½ ì²´í¬ ë° ê°±ì‹ 
        
        Returns:
            ê°±ì‹  ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        self.last_check_time = datetime.now()
        
        logger.info(f"{'='*60}")
        logger.info(f"Checking for data updates... ({self.last_check_time.isoformat()})")
        logger.info(f"{'='*60}")
        
        try:
            # JSONL íŒŒì¼ ëª©ë¡
            jsonl_files = list(self.data_dir.glob("*.jsonl"))
            
            if not jsonl_files:
                logger.warning(f"No JSONL files found in {self.data_dir}")
                return {"status": "no_files", "message": "No data files found"}
            
            # ë³€ê²½ëœ íŒŒì¼ ì°¾ê¸°
            changed_files = self.tracker.get_changed_files(jsonl_files)
            
            if not changed_files:
                logger.info("âœ… No changes detected")
                return {
                    "status": "no_changes",
                    "checked_files": len(jsonl_files),
                    "duration": time.time() - start_time
                }
            
            logger.info(f"ğŸ“ Changes detected in {len(changed_files)} files:")
            for file in changed_files:
                logger.info(f"  - {file.name}")
            
            # ë³€ê²½ëœ íŒŒì¼ë“¤ ì¬ì„ë² ë”©
            refresh_results = {}
            
            for file_path in changed_files:
                logger.info(f"Re-embedding: {file_path.name}")
                try:
                    stats = self.pipeline.process_single_file(file_path, reset=False)
                    refresh_results[file_path.name] = stats.to_dict()
                except Exception as e:
                    logger.error(f"Failed to re-embed {file_path.name}: {e}")
                    refresh_results[file_path.name] = {"status": "error", "error": str(e)}
            
            # í•´ì‹œ ìºì‹œ ì €ì¥
            self.tracker.save_cache()
            
            self.refresh_count += 1
            duration = time.time() - start_time
            
            result = {
                "status": "refreshed",
                "changed_files": [str(f) for f in changed_files],
                "refresh_count": self.refresh_count,
                "results": refresh_results,
                "duration": round(duration, 2),
                "timestamp": self.last_check_time.isoformat()
            }
            
            logger.info(f"{'='*60}")
            logger.info(f"âœ… Refresh complete: {len(changed_files)} files in {duration:.2f}s")
            logger.info(f"{'='*60}")
            
            return result
            
        except Exception as e:
            logger.error(f"Check and refresh failed: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "duration": time.time() - start_time
            }
    
    async def force_refresh_all(self) -> Dict[str, Any]:
        """
        ê°•ì œ ì „ì²´ ì¬ì„ë² ë”©
        
        Returns:
            ê°±ì‹  ê²°ê³¼
        """
        logger.warning("ğŸ”„ Force refresh all - resetting vectorstore")
        
        start_time = time.time()
        
        try:
            results = self.pipeline.process_all_frameworks(reset=True)
            
            # ëª¨ë“  íŒŒì¼ í•´ì‹œ ì—…ë°ì´íŠ¸
            jsonl_files = list(self.data_dir.glob("*.jsonl"))
            for file_path in jsonl_files:
                self.tracker.compute_hash(file_path)  # í•´ì‹œ ì—…ë°ì´íŠ¸
            
            self.tracker.save_cache()
            
            self.refresh_count += 1
            duration = time.time() - start_time
            
            stats = {
                framework: result.to_dict()
                for framework, result in results.items()
            }
            
            return {
                "status": "force_refreshed",
                "frameworks": stats,
                "total_documents": self.pipeline.chroma.count(),
                "duration": round(duration, 2),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Force refresh failed: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e)
            }
    
    def get_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ì¡°íšŒ"""
        return {
            "is_running": self.is_running,
            "last_check_time": self.last_check_time.isoformat() if self.last_check_time else None,
            "refresh_count": self.refresh_count,
            "check_interval": self.check_interval,
            "data_dir": str(self.data_dir),
            "tracked_files": len(self.tracker.hashes),
            "vectorstore_count": self.pipeline.chroma.count()
        }


# ì „ì—­ íƒœìŠ¤í¬ ì¸ìŠ¤í„´ìŠ¤
_refresh_task: Optional[VectorRefreshTask] = None


def get_refresh_task(
    data_dir: str = "./backend/src/ai_assits/esg_mapping/data",
    auto_start: bool = False
) -> VectorRefreshTask:
    """
    ê°±ì‹  íƒœìŠ¤í¬ ì‹±ê¸€í†¤ ë°˜í™˜
    
    Args:
        data_dir: JSONL ë°ì´í„° ë””ë ‰í† ë¦¬
        auto_start: Trueë©´ ìë™ ì‹œì‘
    """
    global _refresh_task
    
    if _refresh_task is None:
        _refresh_task = VectorRefreshTask(
            data_dir=Path(data_dir),
            auto_start=auto_start
        )
    
    return _refresh_task


def reset_refresh_task():
    """íƒœìŠ¤í¬ ì´ˆê¸°í™” (í…ŒìŠ¤íŠ¸ìš©)"""
    global _refresh_task
    if _refresh_task:
        _refresh_task.stop()
    _refresh_task = None

