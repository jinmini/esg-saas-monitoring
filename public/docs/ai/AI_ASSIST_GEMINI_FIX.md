# Gemini API ë¹ˆ ì‘ë‹µ ë¬¸ì œ í•´ê²°

## ğŸ” ë¬¸ì œ ì§„ë‹¨

### ì¦ìƒ
```
ERROR - Empty response from Gemini API
WARNING - Generation attempt 1 failed: Empty response from API
```

### ì›ì¸
```
Finish reason: FinishReason.MAX_TOKENS
response.text value: None
```

**ê·¼ë³¸ ì›ì¸:**
- `max_output_tokens=200`ì´ ë„ˆë¬´ ì‘ì•„ì„œ ì‘ë‹µì´ ì¤‘ê°„ì— ì˜ë¦¼
- Gemini APIëŠ” ë¶ˆì™„ì „í•œ ì‘ë‹µì— ëŒ€í•´ `response.text = None` ë°˜í™˜
- `google-genai` SDKëŠ” MAX_TOKENSë¡œ ì¤‘ë‹¨ëœ ê²½ìš° `response.text` ëŒ€ì‹  `response.candidates[0].content.parts[0].text`ì— ë¶€ë¶„ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  ìˆ˜ ìˆìŒ

---

## âœ… í•´ê²° ë°©ë²•

### 1. `config.py` - ì¶©ë¶„í•œ í† í° ìˆ˜ ì„¤ì •

```python
# Gemini API
GEMINI_API_KEY: str
GEMINI_MODEL: str = "gemini-2.5-flash"
GEMINI_MAX_OUTPUT_TOKENS: int = 2048  # â† ESG ë§¤í•‘ ë¶„ì„ì— ì¶©ë¶„í•œ í† í° ìˆ˜
GEMINI_TEMPERATURE: float = 0.3
```

**ì„¤ëª…:**
- ESG ë§¤í•‘ì€ ì—¬ëŸ¬ í›„ë³´ë¥¼ ë¶„ì„í•˜ê³  JSON ì‘ë‹µì„ ìƒì„±í•˜ë¯€ë¡œ ìµœì†Œ 1024 í† í° ì´ìƒ í•„ìš”
- 2048 í† í°ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì¶©ë¶„í•¨
- gemini-2.5-flash Free tierì˜ TPM(250,000)ì„ ê³ ë ¤í•˜ë©´ ì ì • ìˆ˜ì¤€

---

### 2. `gemini_client.py` - MAX_TOKENS ì²˜ë¦¬ ë¡œì§ ì¶”ê°€

#### A. ì‘ë‹µ ì¶”ì¶œ ê°œì„ 

```python
# ë°©ë²• 1: response.text (ì¼ë°˜ì )
if hasattr(response, 'text') and response.text:
    text = response.text.strip()
    logger.debug(f"Extracted text via response.text: {len(text)} chars")

# ë°©ë²• 2: response.candidates[0].content.parts[0].text
# (MAX_TOKENSë¡œ ì¤‘ë‹¨ëœ ê²½ìš° response.textëŠ” Noneì¼ ìˆ˜ ìˆìŒ)
elif hasattr(response, 'candidates') and response.candidates:
    if hasattr(response.candidates[0], 'content'):
        content = response.candidates[0].content
        if hasattr(content, 'parts') and content.parts:
            if hasattr(content.parts[0], 'text'):
                text = content.parts[0].text
                if text:  # None ì²´í¬
                    text = text.strip()
                    logger.debug(f"Extracted text via candidates: {len(text)} chars")
```

**ì„¤ëª…:**
- `response.text`ê°€ Noneì´ì–´ë„ `candidates[0].content.parts[0].text`ì—ì„œ ë¶€ë¶„ ì‘ë‹µ ì¶”ì¶œ ì‹œë„
- ë‘ ë°©ë²•ì„ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ì¶”ì¶œ ì„±ê³µë¥  í–¥ìƒ

#### B. MAX_TOKENS ê²½ê³ 

```python
# MAX_TOKENSë¡œ ì¤‘ë‹¨ëœ ê²½ìš° ê²½ê³ 
if finish_reason and 'MAX_TOKENS' in str(finish_reason):
    if text:
        logger.warning(f"Response truncated due to MAX_TOKENS limit ({self.max_output_tokens})")
        logger.warning("Consider increasing max_output_tokens for complete responses")
    else:
        logger.error(f"Response truncated and empty due to MAX_TOKENS limit")
        raise ValueError(f"Response truncated by MAX_TOKENS ({self.max_output_tokens})")
```

**ì„¤ëª…:**
- MAX_TOKENSë¡œ ì¤‘ë‹¨ëœ ê²½ìš°ë¥¼ ëª…í™•íˆ ê°ì§€í•˜ê³  ë¡œê·¸ ì¶œë ¥
- ë¶€ë¶„ ì‘ë‹µì´ ìˆìœ¼ë©´ ê²½ê³ ë§Œ í‘œì‹œí•˜ê³  ê³„ì† ì§„í–‰
- ì™„ì „íˆ ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬ ë°œìƒ

#### C. `get_gemini_client()` íŒŒë¼ë¯¸í„° í™•ì¥

```python
def get_gemini_client(
    api_key: Optional[str] = None,
    model_name: Optional[str] = None,
    temperature: Optional[float] = None,
    max_output_tokens: Optional[int] = None
) -> GeminiClient:
    # ...
    _gemini_client = GeminiClient(
        api_key=api_key,
        model_name=model_name or "gemini-2.5-flash",
        temperature=temperature or 0.3,
        max_output_tokens=max_output_tokens or 2048  # â† ê¸°ë³¸ê°’ 2048
    )
```

**ì„¤ëª…:**
- Configì—ì„œ ì„¤ì •ì„ ì „ë‹¬ë°›ì„ ìˆ˜ ìˆë„ë¡ íŒŒë¼ë¯¸í„° ì¶”ê°€
- ê¸°ë³¸ê°’ì„ 2048ë¡œ ì„¤ì •í•˜ì—¬ ì•ˆì „ì„± í™•ë³´

---

### 3. `service.py` - Config ì„¤ì • ì „ë‹¬

```python
def get_esg_mapping_service() -> ESGMappingService:
    global _esg_mapping_service
    
    if _esg_mapping_service is None:
        from src.ai_assist.config import get_ai_config
        config = get_ai_config()
        
        # Gemini í´ë¼ì´ì–¸íŠ¸ë¥¼ configë¡œ ì´ˆê¸°í™”
        from src.ai_assist.core.gemini_client import get_gemini_client
        get_gemini_client(
            api_key=config.GEMINI_API_KEY,
            model_name=config.GEMINI_MODEL,
            temperature=config.GEMINI_TEMPERATURE,
            max_output_tokens=config.GEMINI_MAX_TOKENS  # â† 2048 ì „ë‹¬
        )
        
        _esg_mapping_service = ESGMappingService(...)
```

**ì„¤ëª…:**
- ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œ configì˜ ëª¨ë“  Gemini ì„¤ì •ì„ í´ë¼ì´ì–¸íŠ¸ì— ì „ë‹¬
- ì¤‘ì•™í™”ëœ ì„¤ì • ê´€ë¦¬ë¡œ ì¼ê´€ì„± í™•ë³´

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

### Before (ë¬¸ì œ ìƒí™©)
```python
# test_gemini_api.py - Test 5
max_output_tokens=200  # â† ë„ˆë¬´ ì‘ìŒ

# ê²°ê³¼:
# Finish reason: FinishReason.MAX_TOKENS
# response.text value: None
# âŒ ERROR: 'NoneType' object is not subscriptable
```

### After (í•´ê²°)
```python
# config.py
GEMINI_MAX_OUTPUT_TOKENS: int = 2048

# gemini_client.py
max_output_tokens=2048  # â† ì¶©ë¶„í•œ í† í°

# ì˜ˆìƒ ê²°ê³¼:
# Finish reason: FinishReason.STOP
# response.text: "ESG (Environmental, Social, Governance) reporting standards..."
# âœ… Response generated successfully
```

---

## ğŸ“‹ ë³€ê²½ íŒŒì¼ ìš”ì•½

| íŒŒì¼ | ë³€ê²½ ë‚´ìš© |
|-----|---------|
| `config.py` | `GEMINI_MAX_TOKENS=2048` ì„¤ì • í™•ì¸ |
| `gemini_client.py` | MAX_TOKENS ì²˜ë¦¬ ë¡œì§ ì¶”ê°€, candidates ë°©ì‹ ì¶”ì¶œ ì§€ì› |
| `service.py` | Config ì„¤ì •ì„ Gemini í´ë¼ì´ì–¸íŠ¸ì— ì „ë‹¬ |

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

1. âœ… **í…ŒìŠ¤íŠ¸ ì‹¤í–‰**
   ```powershell
   cd backend
   python scripts/ai/test_esg_mapping.py
   ```

2. âœ… **ë¡œê·¸ í™•ì¸**
   - `Finish reason: STOP` í™•ì¸
   - `Extracted text via response.text` í™•ì¸
   - MAX_TOKENS ê²½ê³ ê°€ ì—†ëŠ”ì§€ í™•ì¸

3. âœ… **ì‹¤ì œ ESG ë§¤í•‘ í…ŒìŠ¤íŠ¸**
   - ê¸´ í…ìŠ¤íŠ¸ë¡œ í…ŒìŠ¤íŠ¸í•˜ì—¬ 2048 í† í°ìœ¼ë¡œ ì¶©ë¶„í•œì§€ í™•ì¸
   - í•„ìš”ì‹œ 4096ìœ¼ë¡œ ì¦ê°€ (ë‹¨, Rate Limit ê³ ë ¤)

---

## ğŸ’¡ ì¶”ê°€ ìµœì í™” ê³ ë ¤ì‚¬í•­

### í† í° ìˆ˜ ì¡°ì • ê°€ì´ë“œ

| ìš©ë„ | ê¶Œì¥ max_tokens | ì„¤ëª… |
|-----|----------------|------|
| ê°„ë‹¨í•œ ë¶„ë¥˜ | 512 | ì§§ì€ JSON ì‘ë‹µ |
| ESG ë§¤í•‘ (í˜„ì¬) | 2048 | ì—¬ëŸ¬ í›„ë³´ ë¶„ì„ + JSON |
| ìƒì„¸ ì„¤ëª… | 4096 | ê¸´ reasoning í¬í•¨ |
| ë³´ê³ ì„œ ìƒì„± | 8192 | ì „ì²´ ë¬¸ì„œ ìƒì„± |

**í˜„ì¬ ì„¤ì • (2048):**
- âœ… ESG ë§¤í•‘ì— ì í•©
- âœ… Free tier TPM(250,000) ë‚´ì—ì„œ ì¶©ë¶„í•œ ìš”ì²­ ê°€ëŠ¥
- âœ… ëŒ€ë¶€ë¶„ì˜ ê²½ìš° MAX_TOKENS ë„ë‹¬ ì—†ìŒ

### Rate Limit ê³„ì‚°
```
TPM = 250,000 (Free tier)
max_output_tokens = 2048
í‰ê·  input_tokens â‰ˆ 1000 (ESG ë§¤í•‘)

ìš”ì²­ë‹¹ í† í° = 1000 + 2048 = 3048
ë¶„ë‹¹ ìµœëŒ€ ìš”ì²­ = 250,000 / 3048 â‰ˆ 82 requests
```

â†’ **í˜„ì¬ RPM ì œí•œ(10)ë³´ë‹¤ TPMì´ ë” ì œí•œì ì´ì§€ ì•ŠìŒ**

---

## ğŸ“š ì°¸ê³ 

- [Gemini API Rate Limits](https://ai.google.dev/gemini-api/docs/rate-limits)
- [google-genai SDK Documentation](https://github.com/googleapis/python-genai)
- [FinishReason Enum](https://ai.google.dev/api/python/google/generativeai/types/FinishReason)

